# Import the build_qa_chain function from your custom rag_chain module
# This function sets up the Retrieval-Augmented Generation (RAG) pipeline
from rag_chain import build_qa_chain

# Build the QA chain using default model settings (since no argument passed here)
qa = build_qa_chain()

# Define a natural language query to test the system
query = "List popular Indian TV shows released after 2020"

# Invoke the QA chain with the query
# This will search the vector database (FAISS) + run the LLM to generate an answer
result = qa.invoke(query)

# Print the answer extracted/generated by the RAG pipeline
print("\n📝 Answer:", result["result"])

# Print the supporting source documents used for the answer
print("\n📂 Sources:")
for doc in result["source_documents"]:
    # Print the first 200 characters of each source document (to avoid huge text blocks)
    print("-", doc.page_content[:200], "...")
